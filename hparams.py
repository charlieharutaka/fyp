import torch.nn as nn
from models.tacotron import DecoderCell

TACOTRON_HP = {
    "encoder_lyric_dim": 256,
    "encoder_pitch_dim": 256,
    "encoder_rhythm_dim": 256,
    "embedding_dim": 256,
    "encoder_n_convolutions": 3,
    "encoder_kernel_size": 5,
    "encoder_p_dropout": 0.5,
    "hidden_dim": 256,
    "attention_dim": 128,
    "attention_rnn_dim": 1024,
    "attention_location_n_filters": 32,
    "attention_location_kernel_size": 31,
    "decoder_rnn_dim": 1024,
    "p_prenet_dropout": 0.1,
    "p_attention_dropout": 0.1,
    "p_decoder_dropout": 0.1,
    "prenet_dim": 128,
    "max_decoder_steps": 1000,
    "stopping_threshold": 0.5,
    "postnet_n_convolutions": 5,
    "postnet_embedding_dim": 512,
    "postnet_kernel_size": 5,
    "postnet_p_dropout": 0.5,
    "decoder_cell": DecoderCell
}

WAVENET_HP = {
    "layers": 10,
    "blocks": 4,
    "in_channels": 1,
    "cond_in_channels": 64,
    "cond_channels": 32,
    "dilation_channels": 32,
    "residual_channels": 32,
    "skip_channels": 256,
    "end_channels": 256,
    "classes": 256,
    "kernel_size": 2,
    "bias": False
}


SODIUM_HP = {
    "embedding_lyric_dim": 256,
    "embedding_pitch_dim": 256,
    "embedding_dim": 256,
    "encoder_prenet_num_convolutions": 3,
    "encoder_prenet_kernel_size": 5,
    "encoder_prenet_activation": nn.Identity(),
    "encoder_p_dropout": 0.1,
    "encoder_use_transformer": True,
    "encoder_transformer_nlayers": 8,
    "encoder_transformer_nhead": 4,
    "encoder_transformer_ff_dim": 1024,
    "encoder_transformer_activation": "relu",
    "encoder_lstm_num_layers": 1,
    "encoder_lstm_zoneout": 0.1,
    "duration_hidden_dim": 256,
    "duration_n_layers": 1,
    "duration_bias": False,
    "range_hidden_dim": 256,
    "range_n_layers": 1,
    "range_bias": True,
    "pos_embedding_dim": 32,
    "pos_embedding_denom": 10000.0,
    "pos_embedding_max_len": 1000,
    "decoder_prenet_n_layers": 2,
    "decoder_prenet_dim": 256,
    "decoder_prenet_activation": nn.ReLU(),
    "decoder_prenet_p_dropout": 0.5,
    "decoder_dim": 1024,
    "decoder_n_layers": 1,
    "output_dim": 128,
    "postnet_num_convolutions": 5,
    "postnet_hidden_features": 512,
    "postnet_kernel_size": 5,
    "postnet_activation": nn.Tanh()
}


SODIUM_LARGE_HP = {
    "embedding_lyric_dim": 512,
    "embedding_pitch_dim": 512,
    "embedding_dim": 512,
    "encoder_prenet_num_convolutions": 3,
    "encoder_prenet_kernel_size": 5,
    "encoder_prenet_activation": nn.Identity(),
    "encoder_p_dropout": 0.1,
    "encoder_use_transformer": False,
    "encoder_transformer_nlayers": 8,
    "encoder_transformer_nhead": 4,
    "encoder_transformer_ff_dim": 1024,
    "encoder_transformer_activation": "relu",
    "encoder_lstm_num_layers": 2,
    "encoder_lstm_zoneout": 0.1,
    "duration_hidden_dim": 512,
    "duration_n_layers": 2,
    "duration_bias": True,
    "range_hidden_dim": 512,
    "range_n_layers": 2,
    "range_bias": True,
    "pos_embedding_dim": 32,
    "pos_embedding_denom": 10000.0,
    "pos_embedding_max_len": 1000,
    "decoder_prenet_n_layers": 2,
    "decoder_prenet_dim": 256,
    "decoder_prenet_activation": nn.ReLU(),
    "decoder_prenet_p_dropout": 0.5,
    "decoder_dim": 1024,
    "decoder_n_layers": 2,
    "output_dim": 128,
    "postnet_num_convolutions": 5,
    "postnet_hidden_features": 512,
    "postnet_kernel_size": 5,
    "postnet_activation": nn.Tanh()
}